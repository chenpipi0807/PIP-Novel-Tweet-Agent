"""
çŸ­å‰§AIç”Ÿæˆç³»ç»Ÿ - ä¸»æ§åˆ¶æµç¨‹
ä»å°è¯´æ–‡æœ¬åˆ°å®Œæ•´è§†é¢‘çš„è‡ªåŠ¨åŒ–ç”Ÿæˆ

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. ä¼ ç»Ÿæ¨¡å¼ (Workflow) - æŒ‰å›ºå®šæµç¨‹æ‰§è¡Œ
2. Agentæ¨¡å¼ (Agent) - æ™ºèƒ½å†³ç­–å’Œè´¨é‡æ§åˆ¶
"""
import os
import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ toolsç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "tools"))

from generate_prompts import PromptGenerator
from kimi_api import KimiAPI

# Agentç›¸å…³å¯¼å…¥
from agent import NovelToVideoAgent
from agent_tools import AgentTools


class VideoGenerator:
    def __init__(self, project_name, novel_text, timbre=None):
        """
        åˆå§‹åŒ–è§†é¢‘ç”Ÿæˆå™¨
        
        Args:
            project_name: é¡¹ç›®åç§°
            novel_text: å°è¯´æ–‡æœ¬
            timbre: éŸ³è‰²æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        """
        self.project_name = project_name
        self.novel_text = novel_text
        self.timbre = timbre
        
        # åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
        self.project_dir = Path("projects") / project_name
        self.audio_dir = self.project_dir / "Audio"
        self.imgs_dir = self.project_dir / "Imgs"
        self.videos_dir = self.project_dir / "Videos"
        
        # åˆ›å»ºæ‰€æœ‰å¿…è¦ç›®å½•
        for dir_path in [self.audio_dir, self.imgs_dir, self.videos_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„
        self.subtitle_file = self.audio_dir / "Subtitles.json"
        self.prompts_file = self.project_dir / "Prompts.json"
        
        print(f"âœ“ é¡¹ç›®åˆå§‹åŒ–: {project_name}")
        print(f"  é¡¹ç›®ç›®å½•: {self.project_dir}")
    
    # Agentè°ƒç”¨çš„ç®€åŒ–æ–¹æ³•å
    def step1_generate_audio(self, skip_if_exists=True):
        """æ­¥éª¤1: ç”ŸæˆéŸ³é¢‘ï¼ˆAgentè°ƒç”¨ï¼‰"""
        return self.step1_generate_audio_and_subtitles(skip_if_exists)
    
    def step3_generate_images(self, skip_if_exists=True):
        """æ­¥éª¤3: ç”Ÿæˆå›¾åƒï¼ˆAgentè°ƒç”¨ï¼‰"""
        return self.step3_generate_images_batch(skip_if_exists)
    
    def step4_compose_video(self, skip_if_exists=True):
        """æ­¥éª¤4: åˆæˆè§†é¢‘ï¼ˆAgentè°ƒç”¨ï¼‰"""
        return self.step4_generate_video()
    
    def step1_generate_audio_and_subtitles(self, skip_if_exists=True):
        """
        æ­¥éª¤1: ç”ŸæˆéŸ³é¢‘å’Œå­—å¹•æ–‡ä»¶
        è°ƒç”¨TTSç³»ç»Ÿç”ŸæˆéŸ³é¢‘å’Œå­—å¹•
        
        Args:
            skip_if_exists: å¦‚æœå­—å¹•æ–‡ä»¶å·²å­˜åœ¨åˆ™è·³è¿‡
        """
        print("\n" + "=" * 70)
        print("æ­¥éª¤ 1/5: ç”ŸæˆéŸ³é¢‘å’Œå­—å¹•")
        print("=" * 70)
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if skip_if_exists and self.subtitle_file.exists():
            print(f"âœ“ å­—å¹•æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            print(f"  æ–‡ä»¶: {self.subtitle_file}")
            return True
        
        # å¯¼å…¥TTSæ¨¡å—
        try:
            from tts_generator import generate_audio_and_subtitles
            
            # è°ƒç”¨TTSç”Ÿæˆ
            result = generate_audio_and_subtitles(
                text=self.novel_text,
                project_name=self.project_name,
                timbre_name=self.timbre
            )
            
            if result and self.subtitle_file.exists():
                print(f"âœ“ å­—å¹•æ–‡ä»¶å·²ç”Ÿæˆ: {self.subtitle_file}")
                
                # è¯»å–å­—å¹•ç»Ÿè®¡ä¿¡æ¯ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
                with open(self.subtitle_file, 'r', encoding='utf-8') as f:
                    subtitles = json.load(f)
                    # æ–°æ ¼å¼ï¼šparent_scenes + child_scenes
                    if 'parent_scenes' in subtitles:
                        total_scenes = subtitles.get('total_parent_scenes', 0)
                        total_children = subtitles.get('total_child_scenes', 0)
                        print(f"  çˆ¶åˆ†é•œï¼ˆå›¾ç‰‡ï¼‰: {total_scenes} ä¸ª")
                        print(f"  å­åˆ†é•œï¼ˆå­—å¹•ï¼‰: {total_children} ä¸ª")
                    else:
                        # æ—§æ ¼å¼ï¼šsubtitles
                        print(f"  æ€»å¥æ•°: {subtitles.get('total_sentences', 0)}")
                    print(f"  æ€»æ—¶é•¿: {subtitles.get('total_duration', 0):.2f}ç§’")
                
                return True
            else:
                print("âŒ å­—å¹•æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ æ­¥éª¤1å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step2_generate_prompts(self, skip_if_exists=True, agent_mode=False):
        """
        æ­¥éª¤2: ç”ŸæˆAIç»˜ç”»æç¤ºè¯
        è°ƒç”¨Kimi APIå¤„ç†å­—å¹•æ–‡ä»¶
        
        Args:
            skip_if_exists: å¦‚æœæç¤ºè¯æ–‡ä»¶å·²å­˜åœ¨åˆ™è·³è¿‡
        """
        print("\n" + "=" * 70)
        print("æ­¥éª¤ 2/5: ç”ŸæˆAIç»˜ç”»æç¤ºè¯")
        print("=" * 70)
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if skip_if_exists and self.prompts_file.exists():
            print(f"âœ“ æç¤ºè¯æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            print(f"  æ–‡ä»¶: {self.prompts_file}")
            return True
        
        if not self.subtitle_file.exists():
            print(f"âŒ å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {self.subtitle_file}")
            return False
        
        try:
            # ä½¿ç”¨PromptGeneratorï¼ˆä¼ é€’agent_modeï¼‰
            generator = PromptGenerator(self.project_name, agent_mode=agent_mode)
            prompts = generator.generate_prompts(str(self.subtitle_file))
            
            # ä¿å­˜åˆ°é¡¹ç›®ç›®å½•
            generator.save_prompts(prompts, str(self.prompts_file))
            
            # æ‰“å°æ‘˜è¦
            generator.print_summary(prompts)
            
            return True
            
        except Exception as e:
            print(f"âŒ æ­¥éª¤2å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step3_generate_images(self, skip_if_exists=True):
        """
        æ­¥éª¤3: ç”Ÿæˆåˆ†é•œå›¾åƒ
        è°ƒç”¨SDXLç”Ÿæˆæ‰€æœ‰åˆ†é•œå›¾ç‰‡
        
        Args:
            skip_if_exists: å¦‚æœå›¾ç‰‡å·²å­˜åœ¨åˆ™è·³è¿‡
        """
        print("\n" + "=" * 70)
        print("æ­¥éª¤ 3/5: ç”Ÿæˆåˆ†é•œå›¾åƒ")
        print("=" * 70)
        
        if not self.prompts_file.exists():
            print(f"âŒ æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {self.prompts_file}")
            return False
        
        try:
            # æ¸…ç†æ˜¾å­˜ï¼ˆç¡®ä¿TTSæ¨¡å‹å·²å¸è½½ï¼‰
            import torch
            import gc
            
            print("æ¸…ç†æ˜¾å­˜...")
            torch.cuda.empty_cache()
            gc.collect()
            
            # è¯»å–æç¤ºè¯
            with open(self.prompts_file, 'r', encoding='utf-8') as f:
                prompts_data = json.load(f)
            
            scene_prompts = prompts_data.get('scene_prompts', [])
            total = len(scene_prompts)
            
            # æ£€æŸ¥å·²ç”Ÿæˆçš„å›¾åƒ
            existing_images = list(self.imgs_dir.glob("scene_*.png"))
            if skip_if_exists and len(existing_images) == total:
                print(f"âœ“ æ‰€æœ‰å›¾åƒå·²å­˜åœ¨ï¼ˆ{total}å¼ ï¼‰ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return True
            
            print(f"éœ€è¦ç”Ÿæˆ {total} å¼ å›¾åƒ")
            if existing_images:
                print(f"  å·²å­˜åœ¨ {len(existing_images)} å¼ ï¼Œå°†è·³è¿‡")
            
            print("åŠ è½½SDXLæ¨¡å‹...")
            
            # å¯¼å…¥SDXLç”Ÿæˆæ¨¡å—
            from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler
            from pathlib import Path
            
            # åŠ è½½æ¨¡å‹ - ä»…ä½¿ç”¨ Prefect Illustrious XL 40
            model_dir = Path("sdxl/models/prefectIllustriousXL_40")
            model_path_file = model_dir / "prefectIllustriousXL_40.safetensors"
            if not model_path_file.exists():
                raise FileNotFoundError("æœªæ‰¾åˆ°æ¨¡å‹: sdxl/models/prefectIllustriousXL_40/prefectIllustriousXL_40.safetensors")
            print("âœ“ ä½¿ç”¨æ¨¡å‹: Prefect Illustrious XL 40")
            model_path = str(model_path_file)
            config_path = str(model_dir)
            
            pipe = StableDiffusionXLPipeline.from_single_file(
                model_path,
                torch_dtype=torch.float16,
                config=config_path,
                local_files_only=True
            ).to("cuda")
            
            # åŠ è½½LoRAï¼ˆä»…DMD2åŠ é€Ÿï¼‰
            dmd2_lora_path = Path("sdxl/models/loras/dmd2_sdxl_4step_lora.safetensors")
            use_dmd2 = False
            if dmd2_lora_path.exists():
                pipe.load_lora_weights(
                    str(dmd2_lora_path.parent), 
                    weight_name=dmd2_lora_path.name,
                    adapter_name="dmd2"
                )
                pipe.set_adapters(["dmd2"], adapter_weights=[0.8])
                use_dmd2 = True
                print("âœ“ DMD2åŠ é€ŸLoRAå·²åŠ è½½ï¼ˆæƒé‡0.8ï¼‰")
            
            # ä½¿ç”¨Euler Ancestralè°ƒåº¦å™¨ï¼ˆåŸå§‹é…ç½®ï¼‰
            pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(
                pipe.scheduler.config
            )
            print("âœ“ ä½¿ç”¨è°ƒåº¦å™¨: Euler Ancestral")
            
            pipe.enable_attention_slicing()
            pipe.enable_vae_slicing()
            
            print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # ç”Ÿæˆå›¾åƒ
            for i, scene in enumerate(scene_prompts, 1):
                index = scene['index']
                prompt = scene['prompt']
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                img_filename = f"scene_{index:04d}.png"
                img_path = self.imgs_dir / img_filename
                
                if skip_if_exists and img_path.exists():
                    print(f"\n[{i}/{total}] åˆ†é•œ {index} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                print(f"\n[{i}/{total}] ç”Ÿæˆåˆ†é•œ {index}...")
                print(f"æç¤ºè¯: {prompt[:80]}...")
                
                # ç”Ÿæˆå›¾åƒï¼ˆä½¿ç”¨éšæœºç§å­ï¼‰
                torch.cuda.empty_cache()
                import random
                seed = random.randint(0, 2**32 - 1)
                generator_obj = torch.Generator(device="cuda").manual_seed(seed)
                
                # è´Ÿé¢è¯ï¼ˆIllustriousä¸“ç”¨ - æ›´å…¨é¢çš„è´¨é‡æ§åˆ¶ï¼‰
                negative_prompt = "lazyneg, lazyhand, child, (censored, mosaic censoring, bar_censor), lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"
                
                # æ·»åŠ è´¨é‡æ ‡ç­¾åˆ°æç¤ºè¯ï¼ˆIllustriousä¸“ç”¨PEæ ¼å¼ï¼‰
                quality_tags = "score_9, score_8_up, score_7_up, masterpiece, best quality, amazing quality, absurdres, newest"
                enhanced_prompt = f"{quality_tags}, BREAK {prompt}"
                
                # ä½¿ç”¨DMD2 LoRAï¼ˆåŸå§‹é…ç½®ï¼‰
                if use_dmd2:
                    # DMD2: 12æ­¥ï¼ŒCFG=2.5ï¼ˆåŸå§‹ç¨³å®šé…ç½®ï¼‰
                    image = pipe(
                        prompt=enhanced_prompt,
                        negative_prompt=negative_prompt,
                        num_inference_steps=12,
                        guidance_scale=2.5,
                        width=1024,
                        height=1024,
                        generator=generator_obj,
                        clip_skip=2
                    ).images[0]
                else:
                    # æ ‡å‡†æ¨¡å¼: 20æ­¥ï¼ŒCFG=7.0ï¼ˆåŸå§‹é…ç½®ï¼‰
                    image = pipe(
                        prompt=enhanced_prompt,
                        negative_prompt=negative_prompt,
                        num_inference_steps=20,
                        guidance_scale=7.0,
                        width=1024,
                        height=1024,
                        generator=generator_obj,
                        clip_skip=2
                    ).images[0]
                
                # ä¿å­˜å›¾åƒ
                img_filename = f"scene_{index:04d}.png"
                img_path = self.imgs_dir / img_filename
                image.save(img_path)
                
                print(f"âœ“ å·²ä¿å­˜: {img_filename}")
            
            print(f"\nâœ“ æ‰€æœ‰å›¾åƒç”Ÿæˆå®Œæˆï¼å…± {total} å¼ ")
            
            # å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜
            print("é‡Šæ”¾æ˜¾å­˜...")
            del pipe
            torch.cuda.empty_cache()
            gc.collect()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ­¥éª¤3å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step4_generate_video(self):
        """
        æ­¥éª¤4: åˆæˆè§†é¢‘
        å°†å›¾ç‰‡å’ŒéŸ³é¢‘åˆæˆä¸ºè§†é¢‘ï¼Œæ·»åŠ éšæœºè½¬åœº
        """
        print("\n" + "=" * 70)
        print("æ­¥éª¤ 4/5: åˆæˆè§†é¢‘")
        print("=" * 70)
        
        # æ£€æŸ¥ffmpeg
        from check_ffmpeg import check_ffmpeg
        if not check_ffmpeg():
            print("\nè¯·å…ˆå®‰è£…ffmpegåå†ç»§ç»­")
            return False
        
        try:
            # å¯¼å…¥è§†é¢‘åˆæˆæ¨¡å—ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒè¿é•œå’Œå­—å¹•ï¼‰
            from video_composer_enhanced import compose_video
            
            video_path = compose_video(
                project_dir=str(self.project_dir),
                subtitle_file=str(self.subtitle_file),
                imgs_dir=str(self.imgs_dir),
                audio_dir=str(self.audio_dir),
                output_dir=str(self.videos_dir)
            )
            
            if video_path:
                print(f"âœ“ è§†é¢‘ç”Ÿæˆå®Œæˆ: {video_path}")
                return True
            else:
                print("âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ æ­¥éª¤4å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        print("\n" + "=" * 70)
        print(f"çŸ­å‰§AIç”Ÿæˆç³»ç»Ÿ - {self.project_name}")
        print("=" * 70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        start_time = datetime.now()
        
        # æ‰§è¡Œå„æ­¥éª¤
        steps = [
            ("ç”ŸæˆéŸ³é¢‘å’Œå­—å¹•", self.step1_generate_audio_and_subtitles),
            ("ç”ŸæˆAIç»˜ç”»æç¤ºè¯", self.step2_generate_prompts),
            ("ç”Ÿæˆåˆ†é•œå›¾åƒ", self.step3_generate_images),
            ("åˆæˆè§†é¢‘", self.step4_generate_video),
        ]
        
        for step_name, step_func in steps:
            if not step_func():
                print(f"\nâŒ æµç¨‹ä¸­æ–­äº: {step_name}")
                return False
        
        # å®Œæˆ
        elapsed = (datetime.now() - start_time).total_seconds()
        print("\n" + "=" * 70)
        print("âœ“ å…¨éƒ¨æµç¨‹å®Œæˆï¼")
        print("=" * 70)
        print(f"æ€»è€—æ—¶: {elapsed:.1f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
        print(f"é¡¹ç›®ç›®å½•: {self.project_dir.absolute()}")
        print(f"è§†é¢‘è¾“å‡º: {self.videos_dir.absolute()}")
        
        return True
    
    def run_with_agent(self, quality_target=0.8, task=None):
        """
        ä½¿ç”¨Agentæ¨¡å¼è¿è¡Œ
        
        Agentä¼šè‡ªä¸»å†³ç­–ã€è¯„ä¼°è´¨é‡ã€ä¼˜åŒ–å‚æ•°
        
        Args:
            quality_target: ç›®æ ‡è´¨é‡åˆ†æ•° (0-1)
            task: Taskå¯¹è±¡ï¼ˆç”¨äºå®æ—¶æ›´æ–°çŠ¶æ€ï¼‰
            
        Returns:
            Agentæ‰§è¡Œç»“æœ
        """
        print("\n" + "=" * 70)
        print("ğŸ¤– Agentæ¨¡å¼å¯åŠ¨")
        print("=" * 70)
        print(f"ç›®æ ‡è´¨é‡: {quality_target}")
        print("Agentå°†è‡ªä¸»å†³ç­–å’Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹...")
        
        start_time = datetime.now()
        
        try:
            # åŠ è½½é…ç½®
            config_path = Path("config.json")
            if not config_path.exists():
                print("âŒ æœªæ‰¾åˆ°config.jsonï¼Œè¯·å…ˆé…ç½®Kimi API Key")
                return None
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            kimi_api_key = config.get("kimi_api_key")
            if not kimi_api_key or kimi_api_key == "sk-your-kimi-api-key":
                print("âŒ è¯·åœ¨config.jsonä¸­é…ç½®æœ‰æ•ˆçš„Kimi API Key")
                return None
            
            # åˆ›å»ºLLMå®¢æˆ·ç«¯
            llm_client = KimiAPI(kimi_api_key)
            
            # åˆ›å»ºå·¥å…·é›†
            agent_tools = AgentTools(self)
            agent_tools.set_llm_client(llm_client)
            
            # åˆ›å»ºAgent
            agent = NovelToVideoAgent(
                llm_client=llm_client,
                tools=agent_tools.get_all_tools(),
                task=task  # ä¼ é€’taskå¯¹è±¡ç”¨äºå®æ—¶æ›´æ–°
            )
            
            # è¿è¡ŒAgent
            result = agent.run(
                project_name=self.project_name,
                novel_text=self.novel_text,
                quality_target=quality_target
            )
            
            # å®Œæˆ
            elapsed = (datetime.now() - start_time).total_seconds()
            print("\n" + "=" * 70)
            print("âœ“ Agentä»»åŠ¡å®Œæˆï¼")
            print("=" * 70)
            print(f"æ€»è€—æ—¶: {elapsed:.1f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
            print(f"è´¨é‡åˆ†æ•°: {result['quality_score']:.2f}")
            print(f"å°è¯•æ¬¡æ•°: {result['attempts']}")
            print(f"é¡¹ç›®ç›®å½•: {self.project_dir.absolute()}")
            if result.get('video_path'):
                print(f"è§†é¢‘è¾“å‡º: {result['video_path']}")
            
            return result
            
        except Exception as e:
            print(f"\nâŒ Agentæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='çŸ­å‰§AIç”Ÿæˆç³»ç»Ÿ')
    parser.add_argument('project_name', help='é¡¹ç›®åç§°')
    parser.add_argument('--text', help='å°è¯´æ–‡æœ¬ï¼ˆç›´æ¥è¾“å…¥ï¼‰')
    parser.add_argument('--file', help='å°è¯´æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', choices=['workflow', 'agent'], default='workflow',
                       help='è¿è¡Œæ¨¡å¼: workflow(ä¼ ç»Ÿæµç¨‹) æˆ– agent(æ™ºèƒ½Agent)')
    parser.add_argument('--quality', type=float, default=0.8,
                       help='Agentæ¨¡å¼çš„ç›®æ ‡è´¨é‡åˆ†æ•° (0-1)')
    
    args = parser.parse_args()
    
    # è·å–å°è¯´æ–‡æœ¬
    if args.text:
        novel_text = args.text
    elif args.file:
        with open(args.file, 'r', encoding='utf-8') as f:
            novel_text = f.read()
    else:
        print("é”™è¯¯: å¿…é¡»æä¾› --text æˆ– --file å‚æ•°")
        return
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = VideoGenerator(args.project_name, novel_text)
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == 'agent':
        print("\nğŸ¤– ä½¿ç”¨Agentæ¨¡å¼")
        generator.run_with_agent(quality_target=args.quality)
    else:
        print("\nğŸ“‹ ä½¿ç”¨ä¼ ç»Ÿå·¥ä½œæµæ¨¡å¼")
        generator.run()


if __name__ == "__main__":
    main()
